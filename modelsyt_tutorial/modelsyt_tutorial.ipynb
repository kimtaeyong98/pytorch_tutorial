{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\taeyong\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0888,  0.0821,  0.0260,  ...,  0.0466, -0.0195, -0.0060],\n",
      "        [-0.0715, -0.0558,  0.0809,  ..., -0.0296,  0.0846, -0.0134],\n",
      "        [-0.0847,  0.0963,  0.0370,  ..., -0.0964, -0.0873,  0.0807],\n",
      "        ...,\n",
      "        [ 0.0484,  0.0958,  0.0576,  ...,  0.0574,  0.0544, -0.0454],\n",
      "        [ 0.0922, -0.0171,  0.0508,  ...,  0.0209,  0.0730, -0.0354],\n",
      "        [-0.0891,  0.0725, -0.0201,  ..., -0.0892, -0.0790, -0.0689]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0395, -0.0649,  0.0941,  0.0727, -0.0256,  0.0160,  0.0894, -0.0452,\n",
      "         0.0895,  0.0790, -0.0695,  0.0263, -0.0595,  0.0515,  0.0245,  0.0935,\n",
      "        -0.0592,  0.0238, -0.0670,  0.0401, -0.0454, -0.0525, -0.0524,  0.0237,\n",
      "        -0.0330,  0.0163, -0.0636,  0.0541, -0.0425,  0.0797,  0.0471,  0.0682,\n",
      "        -0.0733, -0.0293,  0.0314,  0.0242, -0.0853,  0.0856, -0.0802,  0.0084,\n",
      "        -0.0417, -0.0089,  0.0462, -0.0807, -0.0011,  0.0024, -0.0447, -0.0528,\n",
      "        -0.0545, -0.0816,  0.0417,  0.0489,  0.0845,  0.0296,  0.0657,  0.0929,\n",
      "         0.0370,  0.0091,  0.0967,  0.0100,  0.0035, -0.0748,  0.0747, -0.0918,\n",
      "         0.0937,  0.0926,  0.0057,  0.0433,  0.0795, -0.0536, -0.0527, -0.0413,\n",
      "         0.0578,  0.0067,  0.0848,  0.0256,  0.0373, -0.0745,  0.0948,  0.0818,\n",
      "        -0.0258,  0.0600, -0.0431,  0.0256, -0.0060, -0.0468,  0.0945, -0.0458,\n",
      "         0.0450, -0.0478, -0.0640,  0.0497,  0.0736,  0.0801, -0.0537,  0.0715,\n",
      "         0.0399, -0.0610,  0.0228,  0.0500, -0.0591, -0.0906, -0.0508,  0.0013,\n",
      "         0.0600,  0.0703, -0.0548,  0.0971,  0.0544,  0.0965, -0.0233, -0.0081,\n",
      "        -0.0597, -0.0048, -0.0199,  0.0910,  0.0290, -0.0391, -0.0610,  0.0354,\n",
      "         0.0500, -0.0325, -0.0865,  0.0056, -0.0611,  0.0824, -0.0613, -0.0184,\n",
      "        -0.0010,  0.0902,  0.0470, -0.0688,  0.0059, -0.0302, -0.0811, -0.0982,\n",
      "        -0.0924, -0.0781, -0.0491,  0.0921, -0.0208, -0.0615, -0.0173, -0.0374,\n",
      "         0.0917,  0.0134,  0.0311, -0.0133,  0.0874, -0.0207,  0.0609,  0.0955,\n",
      "        -0.0680, -0.0663,  0.0805,  0.0220,  0.0747,  0.0378, -0.0545, -0.0641,\n",
      "        -0.0507,  0.0051, -0.0621, -0.0561,  0.0691, -0.0501, -0.0947,  0.0376,\n",
      "        -0.0623,  0.0201, -0.0852,  0.0787,  0.0922, -0.0806,  0.0965, -0.0869,\n",
      "        -0.0851,  0.0458, -0.0936, -0.0955, -0.0906,  0.0773,  0.0997, -0.0878,\n",
      "         0.0138,  0.0314,  0.0303,  0.0046,  0.0255,  0.0287,  0.0876, -0.0519,\n",
      "         0.0876,  0.0439,  0.0832, -0.0442,  0.0412,  0.0328,  0.0729,  0.0857],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0627,  0.0061,  0.0460,  ..., -0.0593,  0.0005,  0.0261],\n",
      "        [ 0.0153,  0.0513,  0.0242,  ..., -0.0536,  0.0412, -0.0422],\n",
      "        [ 0.0153, -0.0396, -0.0605,  ...,  0.0589,  0.0695,  0.0660],\n",
      "        ...,\n",
      "        [ 0.0255,  0.0160,  0.0690,  ..., -0.0047, -0.0180,  0.0433],\n",
      "        [ 0.0251, -0.0124,  0.0551,  ..., -0.0037, -0.0337,  0.0119],\n",
      "        [ 0.0454,  0.0024,  0.0495,  ...,  0.0370, -0.0683, -0.0258]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0443,  0.0613,  0.0556, -0.0630,  0.0703,  0.0685, -0.0475,  0.0133,\n",
      "        -0.0414, -0.0578], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0627,  0.0061,  0.0460,  ..., -0.0593,  0.0005,  0.0261],\n",
      "        [ 0.0153,  0.0513,  0.0242,  ..., -0.0536,  0.0412, -0.0422],\n",
      "        [ 0.0153, -0.0396, -0.0605,  ...,  0.0589,  0.0695,  0.0660],\n",
      "        ...,\n",
      "        [ 0.0255,  0.0160,  0.0690,  ..., -0.0047, -0.0180,  0.0433],\n",
      "        [ 0.0251, -0.0124,  0.0551,  ..., -0.0037, -0.0337,  0.0119],\n",
      "        [ 0.0454,  0.0024,  0.0495,  ...,  0.0370, -0.0683, -0.0258]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0443,  0.0613,  0.0556, -0.0630,  0.0703,  0.0685, -0.0475,  0.0133,\n",
      "        -0.0414, -0.0578], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)  # 전체 모델\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)  # 레이어 1개\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():  # 파라미터\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():  # 레이어1개의 파라미터\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.2646, 0.8382, 0.4898]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.1076,  0.4995,  0.3844],\n",
      "        [-0.3900, -0.4231,  0.1710]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3462,  0.4930], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[0.2323, 0.1189]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)  # 선형 레이어\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')  # m개의 입력과 n개의 출력이 존재하면, 가중치 행열은 m x n 행렬\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)  # 1개의 이미지, 6개의 출력 피처, 5x5 커널\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)  # 6개의 피처 , 16개의 출력 피처, 3x3 커널\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension  # 선형 레이어이기 때문에 1차원으로 넣어야함\n",
    "        self.fc2 = torch.nn.Linear(120, 84)  # 120 -> 84\n",
    "        self.fc3 = torch.nn.Linear(84, 10)  # 84 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))  # 차원 변경 for 선형레이어\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3203, 0.1013, 0.0721, 0.8973, 0.0353, 0.0339],\n",
      "         [0.0793, 0.2894, 0.8718, 0.4366, 0.2397, 0.0397],\n",
      "         [0.4559, 0.1840, 0.3472, 0.6394, 0.7621, 0.5223],\n",
      "         [0.1764, 0.2733, 0.9393, 0.8112, 0.7139, 0.6021],\n",
      "         [0.2402, 0.6107, 0.2227, 0.3860, 0.0218, 0.9079],\n",
      "         [0.8984, 0.7145, 0.5481, 0.3867, 0.3925, 0.3371]]])\n",
      "tensor([[[0.8718, 0.8973],\n",
      "         [0.9393, 0.9079]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[13.0077, 24.6785, 17.9283,  7.2547],\n",
      "         [ 5.6251, 12.5292, 15.4948,  5.4185],\n",
      "         [17.6945,  7.0986, 20.3522, 16.3436],\n",
      "         [ 9.8474, 23.7154, 15.8247,  8.8876]]])\n",
      "tensor(13.8563)\n",
      "tensor([[[-0.4230,  1.3989,  0.3451, -1.3210],\n",
      "         [-0.9471,  0.6316,  1.3097, -0.9943],\n",
      "         [ 0.4654, -1.6581,  0.9980,  0.1947],\n",
      "         [-0.7986,  1.5471,  0.2124, -0.9610]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor(-4.4703e-08, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.2970, 0.1130, 0.0000],\n",
      "         [1.0278, 1.5434, 0.3014, 0.6118],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.9992, 1.0933, 1.1001]]])\n",
      "tensor([[[1.0694, 0.2970, 0.1130, 0.4142],\n",
      "         [1.0278, 1.5434, 0.0000, 0.0000],\n",
      "         [0.5629, 0.6245, 0.6481, 0.4399],\n",
      "         [0.7664, 0.0000, 1.0933, 1.1001]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)  # 확률적으로 뉴런 비활성화 - 과적합 방지\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c4cb31333e8aa59cbff2148c1763edacdfeefdbe5c2dd2a934b744b24310c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
